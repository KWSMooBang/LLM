{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU 메모리 분해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  0.000 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    if torch.cuda.is_available():\n",
    "        used_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "        print(f\"GPU 메모리 사용량:  {used_memory:.3f} GB\")\n",
    "    else:\n",
    "        print(\"런타임 유형을 GPU로 변경하세요.\")\n",
    "\n",
    "print_gpu_utilization()\n",
    "# GPU 메모리 사용량:  0.000 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woosungkim/anaconda3/envs/machine_learning/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9191ae54e91547a9a0caa9f338a63133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  2.599 GB\n",
      "모델 파라미터 데이터 타입:  torch.float16\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "def load_model_and_tokenizer(model_name, peft=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if peft is None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype='auto', device_map={\"\": 0})\n",
    "    elif peft == 'lora':\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype='auto', device_map={\"\": 0})\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            target_modules=['query_key_value'],\n",
    "            lora_dropout=0.05,\n",
    "            bias='none',\n",
    "            task_type='CAUSAL_LM',\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "    elif peft == 'qlora':\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            target_modules=['query_key_value'],\n",
    "            lora_dropout=0.05,\n",
    "            bias='none',\n",
    "            task_type='CAUSAL_LM'\n",
    "        )\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type='nf4',\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map={\"\": 0})\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "    print_gpu_utilization()\n",
    "    return model, tokenizer\n",
    "\n",
    "model_name = \"EleutherAI/polyglot-ko-1.3b\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "print(\"모델 파라미터 데이터 타입: \", model.dtype)\n",
    "# GPU 메모리 사용량:  2.599 GB\n",
    "# 모델 파라미터 데이터 타입:  torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def estimate_memory_of_gradients(model):\n",
    "    total_memory = 0\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            total_memory += param.grad.nelement() * param.grad.element_size()\n",
    "    return total_memory\n",
    "\n",
    "def estimate_memory_of_optimizer(optimizer):\n",
    "    total_memory = 0\n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if torch.is_tensor(v):\n",
    "                total_memory += v.nelement() * v.element_size()\n",
    "    return total_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, training_args):\n",
    "    if training_args.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    \n",
    "    train_dataloader = DataLoader(dataset, batch_size=training_args.per_device_train_batch_size)\n",
    "    optimizer = AdamW(model.parameters())\n",
    "    model.train()\n",
    "    gpu_utilization_printed = False\n",
    "\n",
    "    gradients_memory = 0\n",
    "    optimizer_memory = 0 \n",
    "\n",
    "    for step, batch in enumerate(train_dataloader, start=1):\n",
    "        batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss = loss / training_args.gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if step % training_args.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            gradients_memory = estimate_memory_of_gradients(model)\n",
    "            optimizer_memory = estimate_memory_of_optimizer(optimizer)\n",
    "            if not gpu_utilization_printed:\n",
    "                print_gpu_utilization()\n",
    "                gpu_utilization_printed = True\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        print(f\"그레이디언트 메모리 사용량: {gradients_memory / (1024 ** 3):.3f} GB\")\n",
    "        print(f\"옵티마이저 상태의 메모리 사용량: {optimizer_memory / (1024 ** 3):.3f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "def make_dummy_dataset():\n",
    "    seq_len, dataset_size = 256, 64\n",
    "    dummy_data = {\n",
    "        'input_ids': np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "        'labels': np.random.randint(100, 30000, (dataset_size, seq_len))\n",
    "    }\n",
    "    dataset = Dataset.from_dict(dummy_data)\n",
    "    dataset.set_format('pt')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc \n",
    "\n",
    "def cleanup():\n",
    "    if 'model' in globals():\n",
    "        del globals()['model']\n",
    "    if 'dataset' in globals():\n",
    "        del globals()['dataset']\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:01:01.696966: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-12 15:01:01.697021: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-12 15:01:01.697832: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-12 15:01:01.702162: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-12 15:01:04.323669: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "def gpu_memory_experiment(\n",
    "    batch_size,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=False,\n",
    "    model_name='EleutherAI/polyglot-ko-1.3b',\n",
    "    peft=None\n",
    "):\n",
    "    print(f\"배치 크기: {batch_size}\")\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name, peft=peft)\n",
    "    if gradient_checkpointing == True or peft == 'qlora':\n",
    "        model.config.use_cache = False\n",
    "    \n",
    "    dataset = make_dummy_dataset()\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        gradient_checkpointing=gradient_checkpointing,\n",
    "        output_dir='../results',\n",
    "        num_train_epochs=1\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        train_model(model, dataset, training_args)\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(e)\n",
    "        else:\n",
    "            raise e\n",
    "    finally:\n",
    "        del model, dataset\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  0.000 GB\n"
     ]
    }
   ],
   "source": [
    "cleanup()\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배치 크기: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woosungkim/anaconda3/envs/machine_learning/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06990ae05c0f41b89ab123dbc48909ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  2.599 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woosungkim/anaconda3/envs/machine_learning/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  10.586 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "GPU 메모리 사용량:  0.016 GB\n",
      "배치 크기: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woosungkim/anaconda3/envs/machine_learning/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb9a959b7db485bb5955e503821d385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  2.615 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woosungkim/anaconda3/envs/machine_learning/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  11.113 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "GPU 메모리 사용량:  0.016 GB\n",
      "배치 크기: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woosungkim/anaconda3/envs/machine_learning/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0b02fad76f40b4aca88f9f2bd9dc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  2.615 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woosungkim/anaconda3/envs/machine_learning/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  12.164 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "GPU 메모리 사용량:  0.016 GB\n"
     ]
    }
   ],
   "source": [
    "for batch_size in [4, 8, 16]:\n",
    "    gpu_memory_experiment(batch_size)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그레이디언트 누적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  5.623 GB\n"
     ]
    }
   ],
   "source": [
    "cleanup()\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배치 크기: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woosungkim/anaconda3/envs/machine_learning/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34dc06c0e9634831a2d96d2172dcea37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  2.599 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woosungkim/anaconda3/envs/machine_learning/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그레이디언트 메모리 사용량: 0.000 GB\n",
      "옵티마이저 상태의 메모리 사용량: 0.000 GB\n",
      "그레이디언트 메모리 사용량: 0.000 GB\n",
      "옵티마이저 상태의 메모리 사용량: 0.000 GB\n",
      "그레이디언트 메모리 사용량: 0.000 GB\n",
      "옵티마이저 상태의 메모리 사용량: 0.000 GB\n",
      "GPU 메모리 사용량:  10.586 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "GPU 메모리 사용량:  0.016 GB\n"
     ]
    }
   ],
   "source": [
    "gpu_memory_experiment(batch_size=4, gradient_accumulation_steps=4)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그레이디언트 체크포인팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup()\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배치 크기: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woosungkim/anaconda3/envs/machine_learning/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8606ebe24434337be90f50cfe83e858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  2.615 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woosungkim/anaconda3/envs/machine_learning/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/woosungkim/anaconda3/envs/machine_learning/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  10.290 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "그레이디언트 메모리 사용량: 2.481 GB\n",
      "옵티마이저 상태의 메모리 사용량: 4.961 GB\n",
      "GPU 메모리 사용량:  0.016 GB\n"
     ]
    }
   ],
   "source": [
    "gpu_memory_experiment(batch_size=16, gradient_checkpointing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA 학습법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  0.016 GB\n"
     ]
    }
   ],
   "source": [
    "cleanup()\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배치 크기: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woosungkim/anaconda3/envs/machine_learning/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fd608e199c4256aba903ddebd0852a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 1,333,383,168 || trainable%: 0.1180\n",
      "GPU 메모리 사용량:  2.621 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woosungkim/anaconda3/envs/machine_learning/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  4.744 GB\n",
      "그레이디언트 메모리 사용량: 0.006 GB\n",
      "옵티마이저 상태의 메모리 사용량: 0.012 GB\n",
      "그레이디언트 메모리 사용량: 0.006 GB\n",
      "옵티마이저 상태의 메모리 사용량: 0.012 GB\n",
      "그레이디언트 메모리 사용량: 0.006 GB\n",
      "옵티마이저 상태의 메모리 사용량: 0.012 GB\n",
      "그레이디언트 메모리 사용량: 0.006 GB\n",
      "옵티마이저 상태의 메모리 사용량: 0.012 GB\n",
      "GPU 메모리 사용량:  0.016 GB\n"
     ]
    }
   ],
   "source": [
    "gpu_memory_experiment(batch_size=16, peft='lora')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLoRA 학습법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef6ff3bed73409c88ffc72619809912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_nf4 = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=nf4_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  0.945 GB\n",
      "배치 크기: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woosungkim/anaconda3/envs/machine_learning/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4748eaba384243e39e729d56c0c9cd91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,572,864 || all params: 1,333,383,168 || trainable%: 0.1180\n",
      "GPU 메모리 사용량:  2.112 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woosungkim/anaconda3/envs/machine_learning/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/woosungkim/anaconda3/envs/machine_learning/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/woosungkim/anaconda3/envs/machine_learning/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량:  2.651 GB\n",
      "그레이디언트 메모리 사용량: 0.006 GB\n",
      "옵티마이저 상태의 메모리 사용량: 0.012 GB\n",
      "그레이디언트 메모리 사용량: 0.006 GB\n",
      "옵티마이저 상태의 메모리 사용량: 0.012 GB\n",
      "그레이디언트 메모리 사용량: 0.006 GB\n",
      "옵티마이저 상태의 메모리 사용량: 0.012 GB\n",
      "그레이디언트 메모리 사용량: 0.006 GB\n",
      "옵티마이저 상태의 메모리 사용량: 0.012 GB\n",
      "GPU 메모리 사용량:  0.945 GB\n"
     ]
    }
   ],
   "source": [
    "cleanup()\n",
    "print_gpu_utilization()\n",
    "\n",
    "gpu_memory_experiment(batch_size=16, peft='qlora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
